{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/surajpatil/miniconda3/envs/mlfetch/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model and post processing to get fixed size embedding\n",
    "class SentenceTransformerModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A custom PyTorch model for generating sentence embeddings using a transformer model.\n",
    "    The model uses a pre-trained transformer (default: all-MiniLM-L6-v2) from the Hugging Face library\n",
    "    and applies mean pooling to obtain a fixed-length sentence embedding.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='sentence-transformers/all-MiniLM-L6-v2'):\n",
    "        \"\"\"\n",
    "        Initializes the SentenceTransformerModel by loading the specified transformer model and tokenizer.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): The model name or path for the pre-trained transformer model.\n",
    "                               Default is 'sentence-transformers/all-MiniLM-L6-v2'.\n",
    "        \"\"\"\n",
    "        super(SentenceTransformerModel, self).__init__()\n",
    "        \n",
    "        # Load the pre-trained transformer model and tokenizer\n",
    "        self.transformer = AutoModel.from_pretrained(model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        \"\"\"\n",
    "        Applies mean pooling on the token embeddings, considering the attention mask.\n",
    "        The attention mask ensures that padding tokens do not affect the averaging process.\n",
    "        \n",
    "        Args:\n",
    "            model_output (tuple): The output of the transformer model containing hidden states.\n",
    "            attention_mask (tensor): The attention mask that specifies which tokens are real and which are padding.\n",
    "        \n",
    "        Returns:\n",
    "            tensor: The sentence embedding after applying mean pooling.\n",
    "        \"\"\"\n",
    "        token_embeddings = model_output[0]  # First element is the token embeddings (hidden states)\n",
    "        \n",
    "        # Expand attention mask to match the size of token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        \n",
    "        # Compute the sum of token embeddings, weighted by the attention mask\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "        \n",
    "        # Avoid division by zero by clamping the sum of the mask to a minimum value\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        \n",
    "        # Return the average of the embeddings (mean pooling)\n",
    "        return sum_embeddings / sum_mask\n",
    "    \n",
    "    def forward(self, sentences):\n",
    "        \"\"\"\n",
    "        Forward pass of the model: encodes input sentences into fixed-length sentence embeddings.\n",
    "        \n",
    "        Args:\n",
    "            sentences (list of str): A list of sentences to be encoded into embeddings.\n",
    "        \n",
    "        Returns:\n",
    "            tensor: A tensor containing the sentence embeddings.\n",
    "        \"\"\"\n",
    "        # Tokenize the input sentences, padding and truncating as necessary, and return PyTorch tensors\n",
    "        encoded_input = self.tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "        \n",
    "        # Pass the tokenized inputs through the transformer model to get token-level embeddings\n",
    "        model_output = self.transformer(**encoded_input)\n",
    "        \n",
    "        # Perform mean pooling to aggregate the token embeddings into fixed-length sentence embeddings\n",
    "        sentence_embeddings = self.mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "        \n",
    "        # Return the sentence embeddings\n",
    "        return sentence_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final answer: tensor([[-0.1096,  0.1359, -0.0030,  ..., -0.1648,  0.2019,  0.2012],\n",
      "        [-0.4113, -0.3770,  0.0335,  ..., -0.4212, -0.3258, -0.1551],\n",
      "        [-0.0059,  0.4075,  0.7919,  ..., -0.1142, -0.4333,  0.3434]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Inference run on test examples\n",
    "sentences = ['Hello World!', 'My name is Suraj3##$', 'Today is a wonderful day, but not great weather though!']\n",
    "# Make object of the model\n",
    "model = SentenceTransformerModel()\n",
    "# Get the outputs\n",
    "embed = model.forward(sentences)\n",
    "print(f'Final answer: {embed}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlfetch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
